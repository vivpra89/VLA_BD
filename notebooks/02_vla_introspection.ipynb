{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_0"
      },
      "source": [
        "# VLA Introspection: Inside SmolVLA\n",
        "\n",
        "This notebook opens up a fine-tuned SmolVLA model and analyzes:\n",
        "1. **Attention maps** — What does the VLA look at in the scene?\n",
        "2. **Action trajectories** — How does flow matching generate action plans?\n",
        "3. **Language conditioning** — Does the model actually use the instruction?\n",
        "4. **Failure analysis** — Where and why does it break?\n",
        "5. **Representation analysis** — What did fine-tuning change in the embedding space?\n",
        "\n",
        "**Prerequisites**: This notebook loads the pre-finetuned checkpoint from [`jadechoghari/smolvla_metaworld`](https://huggingface.co/jadechoghari/smolvla_metaworld) on HuggingFace Hub. No local training is required."
      ],
      "id": "660ccc01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1"
      },
      "source": [
        "## 1. Setup"
      ],
      "id": "c29833ab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_2"
      },
      "source": [
        "# Clone the project repo so all code is available\n",
        "!git clone https://github.com/vivpra89/VLA_BD.git /content/VLA_BD\n",
        "%cd /content/VLA_BD"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c1e782d2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_3"
      },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4c7dafed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_4"
      },
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge -y\n",
        "!cd lerobot && pip install -e \".[smolvla]\"\n",
        "!pip install \"gymnasium==1.1.0\" metaworld matplotlib seaborn scikit-learn"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "af0da5bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_5"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/lerobot/src\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from collections import defaultdict\n",
        "import json, os, copy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "OUTPUT_DIR = \"/content/introspection_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1fcf8bd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_6"
      },
      "source": [
        "## 2. Load models (pretrained + fine-tuned)"
      ],
      "id": "f0b95b55"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_7"
      },
      "source": [
        "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
        "\n",
        "# Load fine-tuned model from HuggingFace Hub\n",
        "FINETUNED_PATH = \"jadechoghari/smolvla_metaworld\"\n",
        "PRETRAINED_PATH = \"lerobot/smolvla_base\"\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "model_finetuned = SmolVLAPolicy.from_pretrained(FINETUNED_PATH)\n",
        "model_finetuned.to(device)\n",
        "model_finetuned.eval()\n",
        "\n",
        "print(\"Loading pretrained (zero-shot) model...\")\n",
        "model_pretrained = SmolVLAPolicy.from_pretrained(PRETRAINED_PATH)\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model_finetuned.parameters()) / 1e6:.1f}M\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0bc65ca1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_8"
      },
      "source": [
        "## 3. Set up Meta-World environment and collect observations"
      ],
      "id": "c565f647"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_9"
      },
      "source": [
        "import metaworld\n",
        "import metaworld.envs\n",
        "\n",
        "def make_metaworld_env(task_name=\"assembly-v3\"):\n",
        "    \"\"\"Create a Meta-World environment and return it with a task.\"\"\"\n",
        "    mt = metaworld.MT1(task_name, seed=42)\n",
        "    env = mt.train_classes[task_name]()\n",
        "    task = mt.train_tasks[0]\n",
        "    env.set_task(task)\n",
        "    return env\n",
        "\n",
        "def collect_episode(env, policy_model, task_desc, max_steps=200, record_internals=False):\n",
        "    \"\"\"Roll out one episode, optionally recording model internals.\"\"\"\n",
        "    obs, info = env.reset()\n",
        "    frames = []\n",
        "    actions_taken = []\n",
        "    rewards = []\n",
        "    internals = []  # will store attention weights, hidden states, etc.\n",
        "    success = False\n",
        "\n",
        "    policy_model.reset()\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        img = env.render()\n",
        "        frames.append(img)\n",
        "\n",
        "        # Build observation dict matching SmolVLA's expected format\n",
        "        obs_dict = build_obs_dict(obs, img, task_desc, device)\n",
        "\n",
        "        if record_internals:\n",
        "            action, step_internals = forward_with_hooks(policy_model, obs_dict)\n",
        "            internals.append(step_internals)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = policy_model.select_action(obs_dict)\n",
        "\n",
        "        action_np = action.cpu().numpy().flatten()\n",
        "        actions_taken.append(action_np)\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(action_np)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if info.get(\"success\", False):\n",
        "            success = True\n",
        "            break\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"actions\": np.array(actions_taken),\n",
        "        \"rewards\": np.array(rewards),\n",
        "        \"success\": success,\n",
        "        \"internals\": internals,\n",
        "        \"num_steps\": len(frames),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_obs_dict(obs_array, image, task_desc, device):\n",
        "    \"\"\"Convert raw Meta-World observation + rendered image into SmolVLA-compatible dict.\n",
        "    \n",
        "    NOTE: This is a simplified adapter. In the full LeRobot pipeline, the env wrapper\n",
        "    handles this. You may need to adjust keys based on the actual model config.\n",
        "    \"\"\"\n",
        "    from lerobot.utils.constants import OBS_STATE, OBS_LANGUAGE_TOKENS, OBS_LANGUAGE_ATTENTION_MASK\n",
        "\n",
        "    # Image: HWC uint8 -> BCHW float [0,1]\n",
        "    img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # State: proprioceptive state from Meta-World obs\n",
        "    state = torch.from_numpy(obs_array[:18].astype(np.float32)).unsqueeze(0).to(device)\n",
        "\n",
        "    # Language tokens: tokenize with the model's processor\n",
        "    tokenizer = model_finetuned.model.vlm_with_expert.processor.tokenizer\n",
        "    tokens = tokenizer(task_desc, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
        "\n",
        "    obs_dict = {\n",
        "        \"observation.images.top\": img_tensor,\n",
        "        OBS_STATE: state,\n",
        "        OBS_LANGUAGE_TOKENS: tokens[\"input_ids\"].to(device),\n",
        "        OBS_LANGUAGE_ATTENTION_MASK: tokens[\"attention_mask\"].bool().to(device),\n",
        "    }\n",
        "    return obs_dict\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "09758aa4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_10"
      },
      "source": [
        "## 4. Attention Visualization: \"What does the VLA look at?\"\n",
        "\n",
        "We hook into the vision encoder (SigLIP) and cross-attention layers to capture attention weights,\n",
        "then overlay them on the input image as heatmaps."
      ],
      "id": "5ee5b45d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_11"
      },
      "source": [
        "class AttentionCapture:\n",
        "    \"\"\"Context manager that hooks into model layers to capture attention weights.\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.hooks = []\n",
        "        self.captured = {}\n",
        "\n",
        "    def __enter__(self):\n",
        "        vlm_model = self.model.model.vlm_with_expert\n",
        "\n",
        "        # Hook into the eager_attention_forward to capture attention probs\n",
        "        original_attn = vlm_model.eager_attention_forward\n",
        "        self._original_attn = original_attn\n",
        "        self._call_count = 0\n",
        "        capture_ref = self\n",
        "\n",
        "        def hooked_attn(attention_mask, batch_size, head_dim, query_states, key_states, value_states):\n",
        "            # Compute attention weights manually to capture them\n",
        "            q = query_states.to(dtype=torch.float32).transpose(1, 2)\n",
        "            k = key_states.to(dtype=torch.float32).transpose(1, 2)\n",
        "\n",
        "            att_weights = torch.matmul(q, k.transpose(2, 3)) * (head_dim ** -0.5)\n",
        "            big_neg = torch.finfo(att_weights.dtype).min\n",
        "            masked = torch.where(attention_mask[:, None, :, :], att_weights, big_neg)\n",
        "            probs = torch.nn.functional.softmax(masked, dim=-1)\n",
        "\n",
        "            capture_ref.captured[f\"attn_layer_{capture_ref._call_count}\"] = probs.detach().cpu()\n",
        "            capture_ref._call_count += 1\n",
        "\n",
        "            # Continue with original computation\n",
        "            return original_attn(attention_mask, batch_size, head_dim, query_states, key_states, value_states)\n",
        "\n",
        "        vlm_model.eager_attention_forward = hooked_attn\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.model.model.vlm_with_expert.eager_attention_forward = self._original_attn\n",
        "\n",
        "\n",
        "def visualize_attention_over_image(image, attn_weights, title=\"Attention Map\"):\n",
        "    \"\"\"Overlay averaged attention weights on the input image.\n",
        "    \n",
        "    attn_weights: (num_heads, query_len, key_len) — we average over heads and\n",
        "    focus on what the action tokens attend to in the image region.\n",
        "    \"\"\"\n",
        "    # Average over heads\n",
        "    attn_avg = attn_weights.mean(dim=0)  # (query_len, key_len)\n",
        "\n",
        "    # The first N tokens in the key dimension correspond to image patches.\n",
        "    # SmolVLA uses 64 visual tokens per image (after pixel shuffle compression).\n",
        "    num_img_tokens = 64\n",
        "    img_grid_size = int(np.sqrt(num_img_tokens))  # 8x8\n",
        "\n",
        "    # Sum attention from all query positions to image tokens\n",
        "    img_attn = attn_avg[:, :num_img_tokens].sum(dim=0)  # (num_img_tokens,)\n",
        "    img_attn = img_attn / img_attn.max()  # normalize to [0,1]\n",
        "\n",
        "    # Reshape to spatial grid\n",
        "    attn_map = img_attn.reshape(img_grid_size, img_grid_size).numpy()\n",
        "\n",
        "    # Resize to image dimensions\n",
        "    from scipy.ndimage import zoom\n",
        "    h, w = image.shape[:2]\n",
        "    attn_resized = zoom(attn_map, (h / img_grid_size, w / img_grid_size), order=1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Input Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(attn_resized, cmap=\"hot\", interpolation=\"bilinear\")\n",
        "    axes[1].set_title(\"Attention Heatmap\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(attn_resized, cmap=\"jet\", alpha=0.5, interpolation=\"bilinear\")\n",
        "    axes[2].set_title(\"Overlay\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "print(\"Attention capture tools defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2915f88d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_12"
      },
      "source": [
        "# Run attention capture on a single observation\n",
        "env = make_metaworld_env(\"assembly-v3\")\n",
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "task_desc = \"assemble the peg into the hole\"\n",
        "\n",
        "obs_dict = build_obs_dict(obs, img, task_desc, device)\n",
        "\n",
        "# Capture attention for fine-tuned model\n",
        "with AttentionCapture(model_finetuned) as cap_ft:\n",
        "    with torch.no_grad():\n",
        "        model_finetuned.reset()\n",
        "        _ = model_finetuned.select_action(obs_dict)\n",
        "\n",
        "# Capture attention for pretrained model\n",
        "with AttentionCapture(model_pretrained) as cap_pt:\n",
        "    with torch.no_grad():\n",
        "        model_pretrained.reset()\n",
        "        _ = model_pretrained.select_action(obs_dict)\n",
        "\n",
        "print(f\"Captured {len(cap_ft.captured)} attention layers (fine-tuned)\")\n",
        "print(f\"Captured {len(cap_pt.captured)} attention layers (pretrained)\")\n",
        "\n",
        "# Visualize early layer attention (layer 2 — low-level features)\n",
        "if \"attn_layer_2\" in cap_ft.captured:\n",
        "    fig = visualize_attention_over_image(\n",
        "        img, cap_ft.captured[\"attn_layer_2\"][0],\n",
        "        title=\"Fine-tuned: Early Layer Attention (Layer 2)\"\n",
        "    )\n",
        "    fig.savefig(f\"{OUTPUT_DIR}/attention_finetuned_layer2.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "if \"attn_layer_2\" in cap_pt.captured:\n",
        "    fig = visualize_attention_over_image(\n",
        "        img, cap_pt.captured[\"attn_layer_2\"][0],\n",
        "        title=\"Pretrained (zero-shot): Early Layer Attention (Layer 2)\"\n",
        "    )\n",
        "    fig.savefig(f\"{OUTPUT_DIR}/attention_pretrained_layer2.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "10855461"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_13"
      },
      "source": [
        "### Attention across task phases\n",
        "\n",
        "Capture attention at different points during an episode to show how focus shifts:\n",
        "- **Reaching**: model should attend to the target object\n",
        "- **Grasping**: attention should shift to gripper-object contact\n",
        "- **Moving**: attention on destination"
      ],
      "id": "48808100"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_14"
      },
      "source": [
        "env = make_metaworld_env(\"assembly-v3\")\n",
        "obs, _ = env.reset()\n",
        "model_finetuned.reset()\n",
        "\n",
        "phase_frames = {\"early (reaching)\": None, \"mid (grasping)\": None, \"late (placing)\": None}\n",
        "phase_attns = {\"early (reaching)\": None, \"mid (grasping)\": None, \"late (placing)\": None}\n",
        "phase_steps = {\"early (reaching)\": 10, \"mid (grasping)\": 50, \"late (placing)\": 100}\n",
        "\n",
        "for step in range(150):\n",
        "    img = env.render()\n",
        "    obs_dict = build_obs_dict(obs, img, \"assemble the peg into the hole\", device)\n",
        "\n",
        "    for phase_name, target_step in phase_steps.items():\n",
        "        if step == target_step:\n",
        "            with AttentionCapture(model_finetuned) as cap:\n",
        "                with torch.no_grad():\n",
        "                    action = model_finetuned.select_action(obs_dict)\n",
        "            phase_frames[phase_name] = img.copy()\n",
        "            # Use a middle layer for the clearest signal\n",
        "            mid_layer = f\"attn_layer_{len(cap.captured) // 2}\"\n",
        "            if mid_layer in cap.captured:\n",
        "                phase_attns[phase_name] = cap.captured[mid_layer][0]\n",
        "            continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        action = model_finetuned.select_action(obs_dict)\n",
        "\n",
        "    action_np = action.cpu().numpy().flatten()\n",
        "    obs, reward, terminated, truncated, info = env.step(action_np)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "# Plot attention across phases\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "for idx, (phase_name, frame) in enumerate(phase_frames.items()):\n",
        "    if frame is not None:\n",
        "        axes[0, idx].imshow(frame)\n",
        "        axes[0, idx].set_title(f\"Step {phase_steps[phase_name]}: {phase_name}\", fontsize=11)\n",
        "        axes[0, idx].axis(\"off\")\n",
        "\n",
        "        if phase_attns[phase_name] is not None:\n",
        "            attn = phase_attns[phase_name].mean(dim=0)\n",
        "            num_img_tokens = min(64, attn.shape[-1])\n",
        "            img_attn = attn[:, :num_img_tokens].sum(dim=0)\n",
        "            img_attn = img_attn / (img_attn.max() + 1e-8)\n",
        "            grid_size = int(np.sqrt(num_img_tokens))\n",
        "            attn_map = img_attn[:grid_size**2].reshape(grid_size, grid_size).numpy()\n",
        "\n",
        "            from scipy.ndimage import zoom\n",
        "            h, w = frame.shape[:2]\n",
        "            attn_resized = zoom(attn_map, (h / grid_size, w / grid_size), order=1)\n",
        "\n",
        "            axes[1, idx].imshow(frame)\n",
        "            axes[1, idx].imshow(attn_resized, cmap=\"jet\", alpha=0.5, interpolation=\"bilinear\")\n",
        "            axes[1, idx].set_title(f\"Attention overlay\", fontsize=11)\n",
        "            axes[1, idx].axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Attention Shift Across Task Phases (assembly-v3)\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTPUT_DIR}/attention_phases.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a66bc1f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_15"
      },
      "source": [
        "## 5. Action Trajectory Visualization: \"What does the VLA plan?\"\n",
        "\n",
        "SmolVLA uses **flow matching** to generate action chunks. Starting from pure noise,\n",
        "it iteratively denoises to produce a sequence of future actions.\n",
        "\n",
        "We visualize:\n",
        "- The denoising trajectory: noise -> clean actions across denoising steps\n",
        "- The predicted action chunk (xyz gripper positions)\n",
        "- Action distribution from multiple noise samples"
      ],
      "id": "c0aae847"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_16"
      },
      "source": [
        "def capture_denoising_trajectory(model, obs_dict, num_noise_samples=10):\n",
        "    \"\"\"Run flow matching inference and capture the intermediate x_t at each denoising step.\"\"\"\n",
        "    model.eval()\n",
        "    flow_model = model.model  # VLAFlowMatching\n",
        "\n",
        "    # Prepare inputs\n",
        "    images, img_masks = model.prepare_images(obs_dict)\n",
        "    state = model.prepare_state(obs_dict)\n",
        "    lang_tokens = obs_dict[\"observation.language_tokens\"]\n",
        "    lang_masks = obs_dict[\"observation.language_attention_mask\"]\n",
        "\n",
        "    bsize = state.shape[0]\n",
        "    actions_shape = (bsize, flow_model.config.chunk_size, flow_model.config.max_action_dim)\n",
        "\n",
        "    all_trajectories = []  # list of (num_denoise_steps, chunk_size, action_dim)\n",
        "\n",
        "    for sample_idx in range(num_noise_samples):\n",
        "        noise = flow_model.sample_noise(actions_shape, device)\n",
        "\n",
        "        # Compute prefix KV cache\n",
        "        prefix_embs, prefix_pad_masks, prefix_att_masks = flow_model.embed_prefix(\n",
        "            images, img_masks, lang_tokens, lang_masks, state=state\n",
        "        )\n",
        "        prefix_att_2d_masks = flow_model.make_att_2d_masks(prefix_pad_masks, prefix_att_masks) if hasattr(flow_model, 'make_att_2d_masks') else None\n",
        "\n",
        "        # Manual denoising loop to capture intermediates\n",
        "        num_steps = flow_model.config.num_steps\n",
        "        dt = -1.0 / num_steps\n",
        "        x_t = noise.clone()\n",
        "        trajectory = [x_t.detach().cpu().clone()]\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            time = 1.0 + step * dt\n",
        "            time_tensor = torch.tensor(time, dtype=torch.float32, device=device).expand(bsize)\n",
        "\n",
        "            suffix_embs, suffix_pad_masks, suffix_att_masks = flow_model.embed_suffix(x_t, time_tensor)\n",
        "            # Simplified: we capture x_t at each step\n",
        "            # In practice you'd run the full denoise_step, but the key insight\n",
        "            # is the progression from noise to structured actions\n",
        "            x_t = x_t + dt * flow_model.denoise_step(\n",
        "                prefix_pad_masks=prefix_pad_masks,\n",
        "                past_key_values=None,  # recompute for simplicity\n",
        "                x_t=x_t,\n",
        "                timestep=time_tensor,\n",
        "            )\n",
        "            trajectory.append(x_t.detach().cpu().clone())\n",
        "\n",
        "        all_trajectories.append(torch.stack(trajectory))  # (num_steps+1, B, chunk, action_dim)\n",
        "\n",
        "    return all_trajectories\n",
        "\n",
        "\n",
        "def plot_denoising_process(trajectories, action_dims=(0, 1, 2), labels=(\"dx\", \"dy\", \"dz\")):\n",
        "    \"\"\"Plot how actions evolve from noise to clean predictions across denoising steps.\"\"\"\n",
        "    fig, axes = plt.subplots(1, len(action_dims), figsize=(5 * len(action_dims), 4))\n",
        "    if len(action_dims) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    traj = trajectories[0]  # (num_steps+1, B, chunk_size, action_dim)\n",
        "    num_steps = traj.shape[0]\n",
        "\n",
        "    for ax_idx, (dim, label) in enumerate(zip(action_dims, labels)):\n",
        "        for chunk_step in range(min(traj.shape[2], 10)):  # show first 10 action steps\n",
        "            values = traj[:, 0, chunk_step, dim].numpy()\n",
        "            alpha = 1.0 - 0.07 * chunk_step\n",
        "            axes[ax_idx].plot(range(num_steps), values, alpha=max(alpha, 0.2), linewidth=1.5)\n",
        "\n",
        "        axes[ax_idx].set_xlabel(\"Denoising Step\")\n",
        "        axes[ax_idx].set_ylabel(f\"Action: {label}\")\n",
        "        axes[ax_idx].set_title(f\"Flow Matching Denoising: {label}\")\n",
        "        axes[ax_idx].grid(True, alpha=0.3)\n",
        "        axes[ax_idx].axhline(y=0, color=\"black\", linewidth=0.5, linestyle=\"--\")\n",
        "\n",
        "    plt.suptitle(\"Noise → Clean Actions (each line = one action step in the chunk)\",\n",
        "                 fontsize=12, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_action_distribution(trajectories, action_dims=(0, 1)):\n",
        "    \"\"\"Show distribution of final predicted actions from different noise seeds.\n",
        "    Tight distribution = model is confident. Wide = uncertain.\n",
        "    \"\"\"\n",
        "    final_actions = []\n",
        "    for traj in trajectories:\n",
        "        final = traj[-1, 0, :, :]  # (chunk_size, action_dim)\n",
        "        final_actions.append(final.numpy())\n",
        "\n",
        "    final_actions = np.array(final_actions)  # (num_samples, chunk_size, action_dim)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(final_actions)))\n",
        "\n",
        "    for i, actions in enumerate(final_actions):\n",
        "        ax.plot(actions[:, action_dims[0]], actions[:, action_dims[1]],\n",
        "                \"o-\", color=colors[i], alpha=0.6, markersize=3, linewidth=1)\n",
        "\n",
        "    # Show mean trajectory\n",
        "    mean_actions = final_actions.mean(axis=0)\n",
        "    ax.plot(mean_actions[:, action_dims[0]], mean_actions[:, action_dims[1]],\n",
        "            \"k*-\", markersize=8, linewidth=2, label=\"Mean trajectory\")\n",
        "\n",
        "    ax.set_xlabel(f\"Action dim {action_dims[0]} (dx)\")\n",
        "    ax.set_ylabel(f\"Action dim {action_dims[1]} (dy)\")\n",
        "    ax.set_title(\"Action Distribution from Multiple Noise Samples\\n(tight = confident, spread = uncertain)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "print(\"Action trajectory tools defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a3ea4314"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_17"
      },
      "source": [
        "# NOTE: The capture_denoising_trajectory function above requires access to the model's\n",
        "# internal denoising loop. If the KV cache handling is complex, use this simpler approach:\n",
        "# Run model.predict_action_chunk() multiple times with different noise seeds.\n",
        "\n",
        "env = make_metaworld_env(\"assembly-v3\")\n",
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "obs_dict = build_obs_dict(obs, img, \"assemble the peg into the hole\", device)\n",
        "\n",
        "# Collect action predictions from multiple noise seeds\n",
        "all_action_chunks = []\n",
        "for seed in range(10):\n",
        "    torch.manual_seed(seed)\n",
        "    model_finetuned.reset()\n",
        "    with torch.no_grad():\n",
        "        chunk = model_finetuned.predict_action_chunk(obs_dict)\n",
        "    all_action_chunks.append(chunk.cpu().numpy())\n",
        "\n",
        "all_action_chunks = np.array(all_action_chunks)  # (10, 1, chunk_size, action_dim)\n",
        "all_action_chunks = all_action_chunks[:, 0, :, :]  # (10, chunk_size, action_dim)\n",
        "\n",
        "# Plot action distribution (xy plane)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "dim_pairs = [(0, 1, \"dx vs dy\"), (0, 2, \"dx vs dz\"), (1, 2, \"dy vs dz\")]\n",
        "for ax, (d1, d2, title) in zip(axes, dim_pairs):\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, 10))\n",
        "    for i in range(10):\n",
        "        ax.plot(all_action_chunks[i, :, d1], all_action_chunks[i, :, d2],\n",
        "                \"o-\", color=colors[i], alpha=0.5, markersize=2, linewidth=1)\n",
        "    mean = all_action_chunks.mean(axis=0)\n",
        "    ax.plot(mean[:, d1], mean[:, d2], \"r*-\", markersize=6, linewidth=2, label=\"Mean\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Predicted Action Chunks (10 noise seeds)\\nTight bundle = confident, spread = uncertain\",\n",
        "             fontsize=13, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTPUT_DIR}/action_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4517c474"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_18"
      },
      "source": [
        "## 6. Language Conditioning Probe: \"Does it actually read the instruction?\"\n",
        "\n",
        "Test whether changing the language instruction actually changes the predicted actions.\n",
        "If the model ignores language, actions should be identical regardless of instruction."
      ],
      "id": "03ae25f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_19"
      },
      "source": [
        "env = make_metaworld_env(\"assembly-v3\")\n",
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "instructions = [\n",
        "    \"assemble the peg into the hole\",\n",
        "    \"turn the dial clockwise\",\n",
        "    \"press the handle from the side\",\n",
        "    \"pick up the red block\",\n",
        "    \"asdkjh random nonsense words xyz\",  # scrambled\n",
        "]\n",
        "\n",
        "# Same visual observation, different instructions\n",
        "torch.manual_seed(42)  # fix noise seed for fair comparison\n",
        "action_by_instruction = {}\n",
        "\n",
        "for instr in instructions:\n",
        "    obs_dict = build_obs_dict(obs, img, instr, device)\n",
        "    torch.manual_seed(42)\n",
        "    model_finetuned.reset()\n",
        "    with torch.no_grad():\n",
        "        chunk = model_finetuned.predict_action_chunk(obs_dict)\n",
        "    action_by_instruction[instr] = chunk.cpu().numpy()[0]  # (chunk_size, action_dim)\n",
        "\n",
        "# Compute pairwise cosine similarity of action chunks\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "flat_actions = np.array([v.flatten() for v in action_by_instruction.values()])\n",
        "sim_matrix = cosine_similarity(flat_actions)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Similarity heatmap\n",
        "short_labels = [instr[:25] + \"...\" if len(instr) > 25 else instr for instr in instructions]\n",
        "sns.heatmap(sim_matrix, xticklabels=short_labels, yticklabels=short_labels,\n",
        "            annot=True, fmt=\".2f\", cmap=\"RdYlGn\", vmin=0, vmax=1, ax=axes[0])\n",
        "axes[0].set_title(\"Cosine Similarity of Action Chunks\\n(same image, different instructions)\", fontsize=11)\n",
        "axes[0].tick_params(axis='x', rotation=30)\n",
        "axes[0].tick_params(axis='y', rotation=0)\n",
        "\n",
        "# Action trajectories overlay\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(instructions)))\n",
        "for i, (instr, actions) in enumerate(action_by_instruction.items()):\n",
        "    label = instr[:30] + \"...\" if len(instr) > 30 else instr\n",
        "    axes[1].plot(actions[:, 0], actions[:, 1], \"o-\", color=colors[i],\n",
        "                 alpha=0.7, markersize=3, linewidth=1.5, label=label)\n",
        "\n",
        "axes[1].set_xlabel(\"Action dim 0 (dx)\")\n",
        "axes[1].set_ylabel(\"Action dim 1 (dy)\")\n",
        "axes[1].set_title(\"Action Trajectories by Instruction\\n(divergence = language matters)\", fontsize=11)\n",
        "axes[1].legend(fontsize=8, loc=\"best\")\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTPUT_DIR}/language_conditioning.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey insight: If the model uses language, different instructions should produce\")\n",
        "print(\"low similarity scores and divergent trajectories. Scrambled text should differ from all.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "78a32f7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_20"
      },
      "source": [
        "## 7. Failure Analysis: \"Where and why does it break?\"\n",
        "\n",
        "Run multiple episodes, categorize failures, and visualize what the model was\n",
        "attending to and predicting at the moment of failure."
      ],
      "id": "c3d8e73b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_21"
      },
      "source": [
        "def run_eval_episodes(env, model, task_desc, n_episodes=20, max_steps=200):\n",
        "    \"\"\"Run multiple episodes and categorize outcomes.\"\"\"\n",
        "    episodes = []\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        model.reset()\n",
        "        frames = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        gripper_positions = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            img = env.render()\n",
        "            frames.append(img)\n",
        "            obs_dict = build_obs_dict(obs, img, task_desc, device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action = model.select_action(obs_dict)\n",
        "\n",
        "            action_np = action.cpu().numpy().flatten()\n",
        "            actions.append(action_np)\n",
        "            gripper_positions.append(obs[:3].copy())  # gripper xyz\n",
        "\n",
        "            obs, reward, terminated, truncated, info = env.step(action_np)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if info.get(\"success\", False):\n",
        "                break\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        episodes.append({\n",
        "            \"success\": info.get(\"success\", False),\n",
        "            \"frames\": frames,\n",
        "            \"actions\": np.array(actions),\n",
        "            \"rewards\": np.array(rewards),\n",
        "            \"gripper_positions\": np.array(gripper_positions),\n",
        "            \"num_steps\": len(frames),\n",
        "            \"total_reward\": sum(rewards),\n",
        "        })\n",
        "        status = \"SUCCESS\" if info.get(\"success\", False) else \"FAIL\"\n",
        "        print(f\"  Episode {ep+1}/{n_episodes}: {status} ({len(frames)} steps, reward={sum(rewards):.2f})\")\n",
        "\n",
        "    return episodes\n",
        "\n",
        "# Run evaluation\n",
        "env = make_metaworld_env(\"assembly-v3\")\n",
        "print(\"Running evaluation episodes...\")\n",
        "episodes = run_eval_episodes(env, model_finetuned, \"assemble the peg into the hole\", n_episodes=20)\n",
        "\n",
        "successes = [e for e in episodes if e[\"success\"]]\n",
        "failures = [e for e in episodes if not e[\"success\"]]\n",
        "print(f\"\\nSuccess rate: {len(successes)}/{len(episodes)} ({100*len(successes)/len(episodes):.0f}%)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "23935406"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_22"
      },
      "source": [
        "# Analyze failure patterns\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Episode length distribution\n",
        "success_lengths = [e[\"num_steps\"] for e in successes]\n",
        "failure_lengths = [e[\"num_steps\"] for e in failures]\n",
        "\n",
        "axes[0, 0].hist(success_lengths, bins=15, alpha=0.7, color=\"green\", label=\"Success\")\n",
        "axes[0, 0].hist(failure_lengths, bins=15, alpha=0.7, color=\"red\", label=\"Failure\")\n",
        "axes[0, 0].set_xlabel(\"Episode Length (steps)\")\n",
        "axes[0, 0].set_ylabel(\"Count\")\n",
        "axes[0, 0].set_title(\"Episode Length Distribution\")\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Reward curves\n",
        "for ep in successes[:3]:\n",
        "    axes[0, 1].plot(np.cumsum(ep[\"rewards\"]), color=\"green\", alpha=0.5)\n",
        "for ep in failures[:3]:\n",
        "    axes[0, 1].plot(np.cumsum(ep[\"rewards\"]), color=\"red\", alpha=0.5)\n",
        "axes[0, 1].set_xlabel(\"Step\")\n",
        "axes[0, 1].set_ylabel(\"Cumulative Reward\")\n",
        "axes[0, 1].set_title(\"Reward Curves (green=success, red=failure)\")\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Gripper trajectories (top-down view, xy plane)\n",
        "for ep in successes[:5]:\n",
        "    pos = ep[\"gripper_positions\"]\n",
        "    axes[1, 0].plot(pos[:, 0], pos[:, 1], \"g-\", alpha=0.4, linewidth=1)\n",
        "    axes[1, 0].plot(pos[0, 0], pos[0, 1], \"go\", markersize=4)\n",
        "    axes[1, 0].plot(pos[-1, 0], pos[-1, 1], \"g^\", markersize=6)\n",
        "\n",
        "for ep in failures[:5]:\n",
        "    pos = ep[\"gripper_positions\"]\n",
        "    axes[1, 0].plot(pos[:, 0], pos[:, 1], \"r-\", alpha=0.4, linewidth=1)\n",
        "    axes[1, 0].plot(pos[0, 0], pos[0, 1], \"ro\", markersize=4)\n",
        "    axes[1, 0].plot(pos[-1, 0], pos[-1, 1], \"r^\", markersize=6)\n",
        "\n",
        "axes[1, 0].set_xlabel(\"X position\")\n",
        "axes[1, 0].set_ylabel(\"Y position\")\n",
        "axes[1, 0].set_title(\"Gripper Trajectories (top-down)\\n(circle=start, triangle=end)\")\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Action magnitude over time (smoothness indicator)\n",
        "for ep in successes[:3]:\n",
        "    magnitudes = np.linalg.norm(ep[\"actions\"][:, :3], axis=1)\n",
        "    axes[1, 1].plot(magnitudes, color=\"green\", alpha=0.5)\n",
        "for ep in failures[:3]:\n",
        "    magnitudes = np.linalg.norm(ep[\"actions\"][:, :3], axis=1)\n",
        "    axes[1, 1].plot(magnitudes, color=\"red\", alpha=0.5)\n",
        "axes[1, 1].set_xlabel(\"Step\")\n",
        "axes[1, 1].set_ylabel(\"Action Magnitude\")\n",
        "axes[1, 1].set_title(\"Action Magnitude Over Time\\n(erratic = loss of control)\")\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Failure Analysis: assembly-v3\", fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTPUT_DIR}/failure_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "aee2c63c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_23"
      },
      "source": [
        "# Show frames from a successful vs failed episode side by side\n",
        "if successes and failures:\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "\n",
        "    best_success = max(successes, key=lambda e: e[\"total_reward\"])\n",
        "    worst_failure = min(failures, key=lambda e: e[\"total_reward\"])\n",
        "\n",
        "    for row, (ep, label) in enumerate([(best_success, \"SUCCESS\"), (worst_failure, \"FAILURE\")]):\n",
        "        n = len(ep[\"frames\"])\n",
        "        indices = [int(i * (n - 1) / 4) for i in range(5)]\n",
        "        for col, idx in enumerate(indices):\n",
        "            axes[row, col].imshow(ep[\"frames\"][idx])\n",
        "            axes[row, col].set_title(f\"{label} — step {idx}\", fontsize=10,\n",
        "                                      color=\"green\" if label == \"SUCCESS\" else \"red\")\n",
        "            axes[row, col].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(\"Best Success vs Worst Failure (assembly-v3)\", fontsize=14, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(f\"{OUTPUT_DIR}/success_vs_failure_frames.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ca959021"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_24"
      },
      "source": [
        "## 8. Representation Analysis: \"What did fine-tuning change?\"\n",
        "\n",
        "Extract hidden states from the VLM backbone for different tasks, and compare\n",
        "how the representation space looks before vs after fine-tuning using t-SNE."
      ],
      "id": "f5941dd4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_25"
      },
      "source": [
        "def extract_hidden_states(model, env, task_desc, n_obs=50):\n",
        "    \"\"\"Extract VLM hidden states for n observations from the environment.\"\"\"\n",
        "    hidden_states = []\n",
        "    obs, _ = env.reset()\n",
        "    model.reset()\n",
        "\n",
        "    for i in range(n_obs):\n",
        "        img = env.render()\n",
        "        obs_dict = build_obs_dict(obs, img, task_desc, device)\n",
        "\n",
        "        # Run the prefix embedding (VLM) and capture the output\n",
        "        images, img_masks = model.prepare_images(obs_dict)\n",
        "        state = model.prepare_state(obs_dict)\n",
        "        lang_tokens = obs_dict[\"observation.language_tokens\"]\n",
        "        lang_masks = obs_dict[\"observation.language_attention_mask\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prefix_embs, _, _ = model.model.embed_prefix(\n",
        "                images, img_masks, lang_tokens, lang_masks, state=state\n",
        "            )\n",
        "            # Use mean-pooled prefix embeddings as the representation\n",
        "            h = prefix_embs.mean(dim=1).cpu().numpy()  # (1, hidden_dim)\n",
        "            hidden_states.append(h[0])\n",
        "\n",
        "        # Step the environment to get diverse observations\n",
        "        with torch.no_grad():\n",
        "            action = model.select_action(obs_dict)\n",
        "        obs, _, terminated, truncated, _ = env.step(action.cpu().numpy().flatten())\n",
        "        if terminated or truncated:\n",
        "            obs, _ = env.reset()\n",
        "            model.reset()\n",
        "\n",
        "    return np.array(hidden_states)  # (n_obs, hidden_dim)\n",
        "\n",
        "# Collect representations for 3 tasks, for both models\n",
        "tasks_for_analysis = [\n",
        "    (\"assembly-v3\", \"assemble the peg into the hole\"),\n",
        "    (\"dial-turn-v3\", \"turn the dial clockwise\"),\n",
        "    (\"handle-press-side-v3\", \"press the handle from the side\"),\n",
        "]\n",
        "\n",
        "n_obs_per_task = 30\n",
        "all_hidden_ft = []  # fine-tuned\n",
        "all_hidden_pt = []  # pretrained\n",
        "all_labels = []\n",
        "\n",
        "for task_name, task_desc in tasks_for_analysis:\n",
        "    print(f\"Extracting representations for {task_name}...\")\n",
        "    env = make_metaworld_env(task_name)\n",
        "\n",
        "    h_ft = extract_hidden_states(model_finetuned, env, task_desc, n_obs=n_obs_per_task)\n",
        "    h_pt = extract_hidden_states(model_pretrained, env, task_desc, n_obs=n_obs_per_task)\n",
        "\n",
        "    all_hidden_ft.append(h_ft)\n",
        "    all_hidden_pt.append(h_pt)\n",
        "    all_labels.extend([task_name.replace(\"-v3\", \"\")] * n_obs_per_task)\n",
        "\n",
        "all_hidden_ft = np.concatenate(all_hidden_ft)\n",
        "all_hidden_pt = np.concatenate(all_hidden_pt)\n",
        "print(f\"Collected {len(all_labels)} representations per model\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2416f338"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_26"
      },
      "source": [
        "# Run t-SNE and compare\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "unique_tasks = list(set(all_labels))\n",
        "task_colors = {task: plt.cm.tab10(i) for i, task in enumerate(unique_tasks)}\n",
        "\n",
        "for ax, (hidden, title) in zip(axes, [\n",
        "    (all_hidden_pt, \"Pretrained (zero-shot)\"),\n",
        "    (all_hidden_ft, \"Fine-tuned on Meta-World\"),\n",
        "]):\n",
        "    tsne = TSNE(n_components=2, perplexity=15, random_state=42, n_iter=1000)\n",
        "    embedded = tsne.fit_transform(hidden)\n",
        "\n",
        "    for task in unique_tasks:\n",
        "        mask = np.array(all_labels) == task\n",
        "        ax.scatter(embedded[mask, 0], embedded[mask, 1],\n",
        "                   c=[task_colors[task]], label=task, alpha=0.7, s=40, edgecolors=\"white\", linewidth=0.5)\n",
        "\n",
        "    ax.set_title(title, fontsize=13, fontweight=\"bold\")\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    ax.set_xlabel(\"t-SNE dim 1\")\n",
        "    ax.set_ylabel(\"t-SNE dim 2\")\n",
        "\n",
        "plt.suptitle(\"Representation Space: Before vs After Fine-Tuning\\n\"\n",
        "             \"(clear clusters after fine-tuning = model learned task structure)\",\n",
        "             fontsize=14, fontweight=\"bold\")\n",
        "plt.tight_layout()\n",
        "fig.savefig(f\"{OUTPUT_DIR}/tsne_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dc50faf3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_27"
      },
      "source": [
        "## 9. Summary of all visualizations\n",
        "\n",
        "All figures saved to the output directory. Use these directly in your presentation slides."
      ],
      "id": "6d0c8e9e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_28"
      },
      "source": [
        "import glob\n",
        "saved_files = sorted(glob.glob(f\"{OUTPUT_DIR}/*.png\"))\n",
        "print(\"Saved visualizations:\")\n",
        "for f in saved_files:\n",
        "    print(f\"  {f}\")\n",
        "\n",
        "print(f\"\\nTotal: {len(saved_files)} figures\")\n",
        "print(\"\\nTo download: Use Colab's file browser (left panel) or run:\")\n",
        "print(f\"  !zip -r /content/introspection_figures.zip {OUTPUT_DIR}\")\n",
        "print(\"  from google.colab import files; files.download('/content/introspection_figures.zip')\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e68cd161"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_29"
      },
      "source": [
        "# Package all figures for download\n",
        "!zip -r /content/introspection_figures.zip {OUTPUT_DIR}\n",
        "from google.colab import files\n",
        "files.download(\"/content/introspection_figures.zip\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "da035e9c"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "name": "02_vla_introspection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}