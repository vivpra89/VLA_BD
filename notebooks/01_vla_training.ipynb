{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "name": "01_vla_training.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": { "id": "title" },
      "source": [
        "# VLA Fine-Tuning: SmolVLA on Meta-World\n",
        "\n",
        "This notebook fine-tunes [SmolVLA](https://huggingface.co/lerobot/smolvla_base) (450M-param Vision-Language-Action model) on [Meta-World](https://github.com/Farama-Foundation/Metaworld) manipulation tasks.\n",
        "\n",
        "We run a **3-way comparison**:\n",
        "1. **Random init** — train SmolVLA from scratch (architecture only, no pretrained weights)\n",
        "2. **Pretrained zero-shot** — evaluate the pretrained SmolVLA directly (no fine-tuning)\n",
        "3. **Fine-tuned** — adapt pretrained SmolVLA to Meta-World tasks\n",
        "\n",
        "**Requirements**: Colab Pro+ with A100 GPU. Training takes ~5 hours per run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "clone_header" },
      "source": [
        "## 0. Clone project repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "clone_repo" },
      "source": [
        "!git clone https://github.com/vivpra89/VLA_BD.git /content/VLA_BD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "setup_header" },
      "source": [
        "## 1. Setup: Install Conda + LeRobot + SmolVLA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "install_conda" },
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "# NOTE: Runtime will restart automatically after this cell. That's expected."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": { "id": "install_lerobot" },
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge -y\n",
        "!cd lerobot && pip install -e \".[smolvla]\"\n",
        "!pip install \"gymnasium==1.1.0\" metaworld wandb matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "wandb_header" },
      "source": [
        "## 2. (Optional) Login to Weights & Biases for training curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "wandb_login" },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "tasks_header" },
      "source": [
        "## 3. Pick Meta-World tasks\n",
        "\n",
        "We select 3 diverse tasks from the MT50 suite:\n",
        "- `assembly-v3` — insert a peg into a hole\n",
        "- `dial-turn-v3` — turn a dial\n",
        "- `handle-press-side-v3` — press a handle from the side\n",
        "\n",
        "These test different manipulation skills: insertion, rotation, and pressing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "config" },
      "source": [
        "TASKS = \"assembly-v3,dial-turn-v3,handle-press-side-v3\"\n",
        "DATASET = \"lerobot/metaworld_mt50\"\n",
        "STEPS_FINETUNE = 20000\n",
        "STEPS_SCRATCH = 20000\n",
        "BATCH_SIZE = 64\n",
        "EVAL_FREQ = 2000\n",
        "EVAL_EPISODES = 5\n",
        "OUTPUT_BASE = \"/content/outputs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "finetune_header" },
      "source": [
        "## 4. Run 1: Fine-tune pretrained SmolVLA\n",
        "\n",
        "Start from `lerobot/smolvla_base` (pretrained on real SO100 robot data) and adapt to Meta-World.\n",
        "This is the standard VLA workflow — take a foundation model, fine-tune on your target domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "train_finetuned" },
      "source": [
        "!cd lerobot && python lerobot/scripts/train.py \\\n",
        "    --policy.path=lerobot/smolvla_base \\\n",
        "    --dataset.repo_id=$DATASET \\\n",
        "    --env.type=metaworld \\\n",
        "    --env.task=$TASKS \\\n",
        "    --batch_size=$BATCH_SIZE \\\n",
        "    --steps=$STEPS_FINETUNE \\\n",
        "    --eval.n_episodes=$EVAL_EPISODES \\\n",
        "    --eval_freq=$EVAL_FREQ \\\n",
        "    --save_freq=$EVAL_FREQ \\\n",
        "    --output_dir=$OUTPUT_BASE/finetuned \\\n",
        "    --job_name=smolvla_finetuned \\\n",
        "    --policy.device=cuda \\\n",
        "    --wandb.enable=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "scratch_header" },
      "source": [
        "## 5. Run 2: Train from scratch (random init)\n",
        "\n",
        "Same architecture, same training, but **no pretrained weights**. This shows how much the pretrained VLM features help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "train_scratch" },
      "source": [
        "!cd lerobot && python lerobot/scripts/train.py \\\n",
        "    --policy.type=smolvla \\\n",
        "    --dataset.repo_id=$DATASET \\\n",
        "    --env.type=metaworld \\\n",
        "    --env.task=$TASKS \\\n",
        "    --batch_size=$BATCH_SIZE \\\n",
        "    --steps=$STEPS_SCRATCH \\\n",
        "    --eval.n_episodes=$EVAL_EPISODES \\\n",
        "    --eval_freq=$EVAL_FREQ \\\n",
        "    --save_freq=$EVAL_FREQ \\\n",
        "    --output_dir=$OUTPUT_BASE/scratch \\\n",
        "    --job_name=smolvla_scratch \\\n",
        "    --policy.device=cuda \\\n",
        "    --wandb.enable=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "eval_header" },
      "source": [
        "## 6. Evaluate all 3 conditions\n",
        "\n",
        "Run evaluation on each task for:\n",
        "- **Pretrained zero-shot** (no training on Meta-World)\n",
        "- **Fine-tuned** (best checkpoint)\n",
        "- **From scratch** (best checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "eval_all" },
      "source": [
        "import subprocess, json, os\n",
        "\n",
        "conditions = {\n",
        "    \"pretrained_zeroshot\": \"lerobot/smolvla_base\",\n",
        "    \"finetuned\": f\"{OUTPUT_BASE}/finetuned/checkpoints/last/pretrained_model\",\n",
        "    \"from_scratch\": f\"{OUTPUT_BASE}/scratch/checkpoints/last/pretrained_model\",\n",
        "}\n",
        "\n",
        "tasks = TASKS.split(\",\")\n",
        "results = {}\n",
        "\n",
        "for cond_name, policy_path in conditions.items():\n",
        "    if not os.path.exists(policy_path) and \"/\" not in policy_path[:5]:\n",
        "        print(f\"Skipping {cond_name}: checkpoint not found at {policy_path}\")\n",
        "        continue\n",
        "    results[cond_name] = {}\n",
        "    for task in tasks:\n",
        "        print(f\"\\n--- Evaluating {cond_name} on {task} ---\")\n",
        "        eval_dir = f\"{OUTPUT_BASE}/eval/{cond_name}/{task}\"\n",
        "        cmd = [\n",
        "            \"python\", \"lerobot/scripts/eval.py\",\n",
        "            f\"--policy.path={policy_path}\",\n",
        "            \"--env.type=metaworld\",\n",
        "            f\"--env.task={task}\",\n",
        "            \"--eval.batch_size=1\",\n",
        "            \"--eval.n_episodes=10\",\n",
        "            f\"--output_dir={eval_dir}\",\n",
        "        ]\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, cwd=\"/content/lerobot\")\n",
        "        print(result.stdout[-500:] if result.stdout else \"No stdout\")\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Error: {result.stderr[-500:]}\")\n",
        "            results[cond_name][task] = 0.0\n",
        "        else:\n",
        "            results[cond_name][task] = \"check eval_dir for results\"\n",
        "\n",
        "print(\"\\n=== All Results ===\")\n",
        "for cond, task_results in results.items():\n",
        "    print(f\"\\n{cond}:\")\n",
        "    for task, score in task_results.items():\n",
        "        print(f\"  {task}: {score}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "viz_header" },
      "source": [
        "## 7. Visualize training curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "visualize" },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json, glob, os\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for idx, (label, output_dir) in enumerate([\n",
        "    (\"Fine-tuned (pretrained init)\", f\"{OUTPUT_BASE}/finetuned\"),\n",
        "    (\"From scratch (random init)\", f\"{OUTPUT_BASE}/scratch\"),\n",
        "]):\n",
        "    log_file = os.path.join(output_dir, \"training_log.json\")\n",
        "    if not os.path.exists(log_file):\n",
        "        log_files = glob.glob(os.path.join(output_dir, \"**/*.json\"), recursive=True)\n",
        "        print(f\"Available log files for {label}: {log_files}\")\n",
        "        continue\n",
        "\n",
        "    with open(log_file) as f:\n",
        "        logs = [json.loads(line) for line in f]\n",
        "\n",
        "    steps = [l[\"step\"] for l in logs if \"loss\" in l]\n",
        "    losses = [l[\"loss\"] for l in logs if \"loss\" in l]\n",
        "\n",
        "    color = \"tab:blue\" if idx == 0 else \"tab:orange\"\n",
        "    axes[0].plot(steps, losses, label=label, color=color, alpha=0.8)\n",
        "\n",
        "axes[0].set_xlabel(\"Training Steps\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Training Loss: Fine-tuned vs From Scratch\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = axes[1]\n",
        "task_names = [\"assembly\", \"dial-turn\", \"handle-press\"]\n",
        "x = np.arange(len(task_names))\n",
        "width = 0.25\n",
        "\n",
        "scratch_rates = [0.0, 0.0, 0.0]\n",
        "zeroshot_rates = [0.0, 0.0, 0.0]\n",
        "finetuned_rates = [0.0, 0.0, 0.0]\n",
        "\n",
        "ax2.bar(x - width, scratch_rates, width, label=\"From scratch\", color=\"tab:red\", alpha=0.8)\n",
        "ax2.bar(x, zeroshot_rates, width, label=\"Pretrained (zero-shot)\", color=\"tab:orange\", alpha=0.8)\n",
        "ax2.bar(x + width, finetuned_rates, width, label=\"Fine-tuned\", color=\"tab:green\", alpha=0.8)\n",
        "\n",
        "ax2.set_xlabel(\"Task\")\n",
        "ax2.set_ylabel(\"Success Rate\")\n",
        "ax2.set_title(\"3-Way Comparison: Success Rate by Task\")\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(task_names)\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 1.0)\n",
        "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_BASE}/training_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved to {OUTPUT_BASE}/training_comparison.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": { "id": "upload_header" },
      "source": [
        "## 8. Upload best checkpoint to HuggingFace Hub\n",
        "\n",
        "Save the fine-tuned model so we can load it in the introspection notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "id": "hf_login" },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": { "id": "hf_upload" },
      "source": [
        "import os\n",
        "HF_USER = os.environ.get(\"HF_USER\", \"YOUR_USERNAME\")\n",
        "\n",
        "!huggingface-cli upload {HF_USER}/smolvla-metaworld-finetuned \\\n",
        "    {OUTPUT_BASE}/finetuned/checkpoints/last/pretrained_model\n",
        "\n",
        "!huggingface-cli upload {HF_USER}/smolvla-metaworld-scratch \\\n",
        "    {OUTPUT_BASE}/scratch/checkpoints/last/pretrained_model\n",
        "\n",
        "print(f\"\\nModels uploaded to:\")\n",
        "print(f\"  https://huggingface.co/{HF_USER}/smolvla-metaworld-finetuned\")\n",
        "print(f\"  https://huggingface.co/{HF_USER}/smolvla-metaworld-scratch\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
